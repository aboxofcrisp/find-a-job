
base on hadoop 3.2.1

HDFS 运行在普通商用服务器中
高度容忍失败
数据存取高吞吐
支持最小POSIX流式数据访问


假定和目标
1.硬件失败是一种常态而不是异常，检测硬件失败和自动恢复
2.数据访问高吞吐而不是低延时
3.大数据集、单个文件GB、TB级别一个集群有上亿个文件。
4.简单一致性模型，仅支持文件内容追加，不支持修改。
5.移动计算比移动数据更高效，本地化读取数据
6.方便在不同的平台中间迁移

NameNode和DataNode  / Clients

采用Master/Slave的主从架构
NameNode 存储Metadata(文件名称，目录，replication)
NameNode 执行Namespace的操作（打开文件，关闭文件，删除文件，重命名文件和目录，数据块分配到DataNode中 ）
DataNode 响应来自客户端对文件的读和写请求，汇报数据块，创建数据块，删除数据块，根据NameNode的replication操作
用户数据绝不流经NameNode节点

文件系统的Namespace
  HDFS支持用户配额和访问许可，不支持硬连接和软连接
  应用程序可以指定一个文件的replication个数，这些信息被存储在NameNode中

数据复制
/users/sameerp/data/part-01,r:2,{2,4,5},...
一个文件的复制因子可以在创建的时候指定，也可以在之后更改
Namenode 周期性接收来自DataNode的心跳和汇报的块数据（DataNode节点有效的全部数据块）

--复制位置的选择
replication的放置决定了HDFS的可靠性和性能，优化replication是区别与其他分布式文件系统的关键点
同一机架下各机器的网路带宽好于不同机架下机器间的网络带宽
最好的选择是replication放置在同一机架上，但是为了增加可靠性通常会把一个replication放置到不同的机架上。
数据的放置策略要在数据可靠性和读性能之间作出妥协

--数据读取选择
 读取请求的DataNode节点靠近Reader
 多数据中心下，优先选择本数据中心的数据块

--安全模式
 在刚开始启动的时候系统进入安全模式，在安全模式下数据块的replication被禁止
 数据块检测具有最小数据块后再加上30秒可以离开安全模式，此后不够replication个数据的数据文件开始拷贝复制数据块使之满足replication个数。

元数据的持久化
Editlog文件记录对文件的每次操作（事务日志）
FsImage 存储在本地文件系统中，存储了namespace，文件系统属性，文件到数据块的映射
namespace的快照和文件块映射关系被存储到内存中
dfs.namenode.checkpoint.period checkpoint间隔时间单位秒
dfs.namenode.checkpoint.txns 事务条数

checkping process:
reference：https://lihuimintu.github.io/2019/09/20/HDFS-HA-checkpoint/

沟通协议
 一切协议都基于TCP/IP协议
 DataNodeProtocol / ClientProtcol  这些协议都封装到RPC调用中

健壮性
NameNode节点故障，DataNode节点故障，网络partition
 -- 磁盘失效，心跳，re-replication 
   每一个DataNode会周期性的发送心跳给Namenode节点
   DataNode节点的故障导致partition低于预期的值，Namenode节点会对这些数据进行处理re-partition
   
 -- 平衡集群
   data-rebalance
 -- 数据完整性
   网络故障，磁盘故障，存储设备故障，软件bug导致数据块故障。 checksum校验数据库是否发生故障。
   客户端获取数据块数据的时候也会校验数据是否正确，未通过校验的数据重新从另一个数据节点获取数据。
   
 -- 元数据磁盘故障
   采用HA方式避免元数据磁盘故障

 -- 快照
 https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html
 可以在一个目录上设置快照，快照不允许嵌套。

 

数据组织
  -- 数据块
  写一次，多次读。典型块大小为128MB

  -- replication管道
   Client ----> OneDN ----> SecondDN ----> ThirdDN
                  |           |              |
               Local File   Local File     Local File
      
可访问性
 API（Java API/REST API）方式，CLI方式，HTTP浏览器方式，NFS网关方式挂载文件系统到本地磁盘

回收空间
  每一个用户都有自己的垃圾目录（/user/<username>/.Trash）
  配置了回收站，删除文件不是立即删除，而是把文件移动到垃圾目录
  
--------
Hadoop 机架感知
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/RackAwareness.html

存储类型和存储策略
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html

HA方法
  非HA情况下
  1.机器崩溃导致服务不可用
  2.NameNode节点的软件或硬件升级导致长时间的停工检查
  
NameNode HA的实现方法
NFS方法  https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html
使用NFS方式实现HA
 Active / StandBy
 采用隔离方法（fencing method）防止脑裂
 

 

Journal方式
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html



NameNode 脑裂
 由于在一个集群中出现了两个NameNode节点所致。 
 避免方法使用fencing method在启动NameNode节点的时候Kill掉已启动的NameNode节点。 
 SSH fence/ Shell fence 两种方法。
 
 
 CAP 理论
 Consistency / Availibility / Partition Tolerance
 一致性   / 可用性 / 分区容忍性
 访问同一份最新的数据副本 /每次请求都获得非错误的响应但不保证数据最新  HDFS采用副本方式提高可用性 / 以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择
 
 根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项
 
 理解CAP理论的最简单方式是想象两个节点分处分区两侧。
 允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。
 除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。
 
 分区容忍性由于在分布式网络中网路故障不可控，分区容忍性问题一直存在。
 
 
 




